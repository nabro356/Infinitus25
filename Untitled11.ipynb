{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nabro356/Infinitus25/blob/main/Untitled11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gtts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAUlOLTI24dh",
        "outputId": "fcbd426f-2cd5-415a-c554-5770ae55cbd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gtts in /usr/local/lib/python3.11/dist-packages (2.5.4)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from gtts) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.11/dist-packages (from gtts) (8.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install facenet-pytorch\n",
        "!pip install --upgrade torch torchvision\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6EguDhj0o9u",
        "outputId": "84bd6104-d257-4474-999a-675769f9aa5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: facenet-pytorch in /usr/local/lib/python3.11/dist-packages (2.6.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch) (1.26.4)\n",
            "Requirement already satisfied: Pillow<10.3.0,>=10.2.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch) (10.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch) (2.32.3)\n",
            "Collecting torch<2.3.0,>=2.2.0 (from facenet-pytorch)\n",
            "  Using cached torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting torchvision<0.18.0,>=0.17.0 (from facenet-pytorch)\n",
            "  Using cached torchvision-0.17.2-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2024.12.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet-pytorch) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet-pytorch) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch) (1.3.0)\n",
            "Using cached torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl (755.6 MB)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "Using cached torchvision-0.17.2-cp311-cp311-manylinux1_x86_64.whl (6.9 MB)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
            "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
            "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
            "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0\n",
            "    Uninstalling torch-2.6.0:\n",
            "      Successfully uninstalled torch-2.6.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0\n",
            "    Uninstalling torchvision-0.21.0:\n",
            "      Successfully uninstalled torchvision-0.21.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 torchvision-0.17.2 triton-2.2.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting torch\n",
            "  Using cached torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.17.2)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting triton==3.2.0 (from torch)\n",
            "  Using cached triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (10.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Using cached torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "Using cached triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
            "Using cached torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.2.0\n",
            "    Uninstalling triton-2.2.0:\n",
            "      Successfully uninstalled triton-2.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
            "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
            "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
            "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
            "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
            "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
            "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.2\n",
            "    Uninstalling torch-2.2.2:\n",
            "      Successfully uninstalled torch-2.2.2\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.17.2\n",
            "    Uninstalling torchvision-0.17.2:\n",
            "      Successfully uninstalled torchvision-0.17.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "facenet-pytorch 2.6.0 requires torch<2.3.0,>=2.2.0, but you have torch 2.6.0 which is incompatible.\n",
            "facenet-pytorch 2.6.0 requires torchvision<0.18.0,>=0.17.0, but you have torchvision 0.21.0 which is incompatible.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\n",
            "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvtx-cu12-12.4.127 torch-2.6.0 torchvision-0.21.0 triton-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "HoLxIhEOPzI4",
        "outputId": "6d88a1ad-ecc2-4219-d896-2a905a97471d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-cloud-vision\n",
            "  Using cached google_cloud_vision-3.9.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2.19.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision) (1.26.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision) (4.25.6)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2024.12.14)\n",
            "Using cached google_cloud_vision-3.9.0-py2.py3-none-any.whl (514 kB)\n",
            "Installing collected packages: google-cloud-vision\n",
            "Successfully installed google-cloud-vision-3.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "15cef623ce624ede9f56999f514f5256"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install google-cloud-vision\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import vision\n",
        "import os\n",
        "\n",
        "# Set up the Google Cloud Vision API client\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/sample_data/vision-api-450017-cbe451515384.json'\n",
        "client = vision.ImageAnnotatorClient()\n",
        "\n",
        "# Load the image file\n",
        "image_path = '/content/sample_data/image3.jpg'\n",
        "with open(image_path, 'rb') as image_file:\n",
        "    content = image_file.read()\n",
        "\n",
        "# Prepare the image for Vision API\n",
        "image = vision.Image(content=content)\n",
        "\n",
        "# Send the request to the API for label detection (describing the scene)\n",
        "response = client.label_detection(image=image)\n",
        "\n",
        "# Check for errors\n",
        "if response.error.message:\n",
        "    raise Exception(f'Error occurred: {response.error.message}')\n",
        "\n",
        "# Print out the description of the scene\n",
        "print(\"Labels detected in the image:\")\n",
        "for label in response.label_annotations:\n",
        "    print(f\"{label.description} (score: {label.score})\")\n",
        "\n",
        "# Optionally, if you want to do more advanced analysis, such as image text detection (OCR)\n",
        "response_text = client.text_detection(image=image)\n",
        "\n",
        "if response_text.text_annotations:\n",
        "    print(\"\\nText detected in the image:\")\n",
        "    for text in response_text.text_annotations:\n",
        "        print(f\"Detected text: {text.description}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8y_7NULQcRU",
        "outputId": "97274881-99a9-4383-c305-3210147e8e77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels detected in the image:\n",
            "Furniture (score: 0.9051231741905212)\n",
            "Hall (score: 0.9024546146392822)\n",
            "Ceiling (score: 0.8823196887969971)\n",
            "Chair (score: 0.8379455804824829)\n",
            "Auditorium (score: 0.6164169311523438)\n",
            "Convention (score: 0.5356208682060242)\n",
            "Community centre (score: 0.5081798434257507)\n",
            "\n",
            "Text detected in the image:\n",
            "Detected text: PLAY\n",
            "Detected text: PLAY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import vision\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Set up the Google Cloud Vision API client\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/sample_data/vision-api-450017-cbe451515384.json'\n",
        "client = vision.ImageAnnotatorClient()\n",
        "\n",
        "GEMINI_API_KEY = 'AIzaSyBjuQHU8GYEPm9Fj0Rrna26-E6zdwXAhLg'\n",
        "\n",
        "def get_labels_from_image(image_path):\n",
        "    \"\"\"Use Google Vision API to get labels from the image.\"\"\"\n",
        "    with open(image_path, 'rb') as image_file:\n",
        "        content = image_file.read()\n",
        "\n",
        "    # Prepare the image for Vision API\n",
        "    image = vision.Image(content=content)\n",
        "\n",
        "    # Send the request to the API for label detection\n",
        "    response = client.label_detection(image=image)\n",
        "\n",
        "    # Check for errors\n",
        "    if response.error.message:\n",
        "        raise Exception(f'Error occurred: {response.error.message}')\n",
        "\n",
        "    # Extract the labels from the response\n",
        "    labels = [label.description for label in response.label_annotations]\n",
        "\n",
        "    return labels\n",
        "\n",
        "def describe_scene_with_gemini(labels):\n",
        "    \"\"\"Generate a scene description using Gemini AI.\"\"\"\n",
        "    # Prepare the prompt with labels\n",
        "    labels_str = \", \".join(labels)\n",
        "    print(f\"Labels detected in the image: {labels_str}\")\n",
        "    prompt = f\"form a small sentence like person XXXX detected with these words: {labels_str}\"\n",
        "\n",
        "    # Set up the request body for Gemini API\n",
        "    url = f'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_API_KEY}'\n",
        "    data = {\n",
        "        \"contents\": [{\n",
        "            \"parts\": [{\n",
        "                \"text\": prompt\n",
        "            }]\n",
        "        }]\n",
        "    }\n",
        "\n",
        "    # Send the request to Gemini API\n",
        "    response = requests.post(url, json=data)\n",
        "\n",
        "    # Check if the response is successful\n",
        "    if response.status_code == 200:\n",
        "        response_data = response.json()\n",
        "        print(response_data)\n",
        "        description = response_data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "        return description\n",
        "    else:\n",
        "        raise Exception(f\"Error occurred in Gemini API: {response.text}\")\n",
        "\n",
        "def main(image_path):\n",
        "    # Get labels from the image using Google Vision API\n",
        "    labels = get_labels_from_image(image_path)\n",
        "\n",
        "    # Generate a description using Gemini AI\n",
        "    scene_description = describe_scene_with_gemini(labels)\n",
        "\n",
        "    # Print the final description\n",
        "    print(\"\\nScene Description:\")\n",
        "    print(scene_description)\n",
        "\n",
        "# Example usage\n",
        "image_path = \"/content/sample_data/image3.jpg\"  # Takes user input for image path\n",
        "main(image_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aoTMO8bVLBD",
        "outputId": "00c1e476-f8bd-47a0-d0ad-7cb751d57d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels detected in the image: Furniture, Hall, Ceiling, Chair, Auditorium, Convention, Community centre\n",
            "{'candidates': [{'content': {'parts': [{'text': 'Person detected in auditorium.\\n'}], 'role': 'model'}, 'finishReason': 'STOP', 'avgLogprobs': -0.23496143023173013}], 'usageMetadata': {'promptTokenCount': 26, 'candidatesTokenCount': 6, 'totalTokenCount': 32, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 26}], 'candidatesTokensDetails': [{'modality': 'TEXT', 'tokenCount': 6}]}, 'modelVersion': 'gemini-1.5-flash'}\n",
            "\n",
            "Scene Description:\n",
            "Person detected in auditorium.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from google.cloud import vision\n",
        "\n",
        "# Set up the Google Cloud Vision API client\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/sample_data/vision-api-450017-cbe451515384.json'\n",
        "\n",
        "client = vision.ImageAnnotatorClient()\n",
        "\n",
        "GEMINI_API_KEY = 'AIzaSyBjuQHU8GYEPm9Fj0Rrna26-E6zdwXAhLg'\n",
        "\n",
        "def get_labels_from_image(image_path):\n",
        "    \"\"\"Use Google Vision API to get labels from the image.\"\"\"\n",
        "    with open(image_path, 'rb') as image_file:\n",
        "        content = image_file.read()\n",
        "\n",
        "    # Prepare the image for Vision API\n",
        "    image = vision.Image(content=content)\n",
        "\n",
        "    # Send the request to the API for label detection\n",
        "    response = client.label_detection(image=image)\n",
        "\n",
        "    # Check for errors\n",
        "    if response.error.message:\n",
        "        raise Exception(f'Error occurred: {response.error.message}')\n",
        "\n",
        "    # Extract the labels from the response\n",
        "    labels = [label.description for label in response.label_annotations]\n",
        "\n",
        "    return labels\n",
        "\n",
        "def describe_scene_with_gemini(labels):\n",
        "    \"\"\"Generate a scene description using Gemini AI.\"\"\"\n",
        "    # Prepare the prompt with labels\n",
        "    labels_str = \", \".join(labels)\n",
        "    print(f\"Labels detected in the image: {labels_str}\")\n",
        "    if (\"Paper\" in labels_str):\n",
        "      prompt =f\"print the text {labels_str}\"\n",
        "    else:\n",
        "      prompt = f\"form a small sentence like person XXXX detected or explain the scene with these words: {labels_str}\"\n",
        "\n",
        "    # Set up the request body for Gemini API\n",
        "    url = f'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_API_KEY}'\n",
        "    data = {\n",
        "        \"contents\": [{\n",
        "            \"parts\": [{\n",
        "                \"text\": prompt\n",
        "            }]\n",
        "\n",
        "        }]\n",
        "    }\n",
        "\n",
        "    # Send the request to Gemini API\n",
        "    response = requests.post(url, json=data)\n",
        "\n",
        "    # Check if the response is successful\n",
        "    if response.status_code == 200:\n",
        "        response_data = response.json()\n",
        "        #print(response_data)\n",
        "        description = response_data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "        return description\n",
        "    else:\n",
        "        raise Exception(f\"Error occurred in Gemini API: {response.text}\")\n",
        "\n",
        "def extract_frames_from_video(video_path, frame_interval):\n",
        "    \"\"\"Extract frames from video at a specific interval (in seconds).\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_rate = cap.get(cv2.CAP_PROP_FPS)  # Get the frame rate of the video\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "    success, frame = cap.read()\n",
        "\n",
        "    while success:\n",
        "        # Calculate the frame number to extract based on frame_interval\n",
        "        if frame_count % int(frame_rate * frame_interval) == 0:  # Every `frame_interval` seconds\n",
        "            frame_filename = f'/tmp/frame_{frame_count}.jpg'\n",
        "            cv2.imwrite(frame_filename, frame)  # Save frame as an image\n",
        "            frames.append(frame_filename)\n",
        "\n",
        "        success, frame = cap.read()\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "def main(video_path, frame_interval):\n",
        "    # Extract frames from the video at the specified interval\n",
        "    frames = extract_frames_from_video(video_path, frame_interval)\n",
        "\n",
        "    # Process each frame\n",
        "    for frame_path in frames:\n",
        "        #print(f\"\\nProcessing frame: {frame_path}\")\n",
        "\n",
        "        # Get labels from the frame using Google Vision API\n",
        "        labels = get_labels_from_image(frame_path)\n",
        "\n",
        "        # Generate a description using Gemini AI\n",
        "        scene_description = describe_scene_with_gemini(labels)\n",
        "\n",
        "        # Print the final description\n",
        "        print(\"\\nScene Description:\")\n",
        "        print(scene_description)\n",
        "\n",
        "# Example usage: Change `frame_interval` to any desired interval (in seconds)\n",
        "video_path = \"/content/sample_data/scene_desc3.mp4\"  # Path to the video file\n",
        "frame_interval = 5  # Set this to the desired interval (e.g., 5 seconds)\n",
        "main(video_path, frame_interval)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ten4SILnqFUv",
        "outputId": "8724f964-fcc3-4994-eca6-99bb13d5725a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels detected in the image: Plaid\n",
            "\n",
            "Scene Description:\n",
            "Plaid detected.\n",
            "\n",
            "Labels detected in the image: Ceiling, Hall, Chair, Crowd\n",
            "\n",
            "Scene Description:\n",
            "Crowd in hall, chair overturned near ceiling.\n",
            "\n",
            "Labels detected in the image: Chair, Crowd, Convention\n",
            "\n",
            "Scene Description:\n",
            "Person detected near chair in convention crowd.\n",
            "\n",
            "Labels detected in the image: Photograph, White, Text, Paper, Number, Paper Product, Document, Ink\n",
            "\n",
            "Scene Description:\n",
            "Photograph, White, Text, Paper, Number, Paper Product, Document, Ink\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from google.cloud import vision\n",
        "\n",
        "# Set up the Google Cloud Vision API client\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/sample_data/vision-api-450017-cbe451515384.json'\n",
        "\n",
        "client = vision.ImageAnnotatorClient()\n",
        "\n",
        "GEMINI_API_KEY = 'AIzaSyBjuQHU8GYEPm9Fj0Rrna26-E6zdwXAhLg'\n",
        "\n",
        "def get_labels_from_image(image_path):\n",
        "    \"\"\"Use Google Vision API to get labels from the image.\"\"\"\n",
        "    with open(image_path, 'rb') as image_file:\n",
        "        content = image_file.read()\n",
        "\n",
        "    # Prepare the image for Vision API\n",
        "    image = vision.Image(content=content)\n",
        "\n",
        "    # Send the request to the API for label detection\n",
        "    response = client.label_detection(image=image)\n",
        "\n",
        "    # Check for errors\n",
        "    if response.error.message:\n",
        "        raise Exception(f'Error occurred: {response.error.message}')\n",
        "\n",
        "    # Extract the labels from the response\n",
        "    labels = [label.description for label in response.label_annotations]\n",
        "\n",
        "    return labels\n",
        "\n",
        "def extract_text_from_image(image_path):\n",
        "    \"\"\"Extract text from an image using Google Vision's document text detection.\"\"\"\n",
        "    with open(image_path, 'rb') as image_file:\n",
        "        content = image_file.read()\n",
        "\n",
        "    # Prepare the image for Vision API\n",
        "    image = vision.Image(content=content)\n",
        "\n",
        "    # Use document text detection for extracting text\n",
        "    response = client.document_text_detection(image=image)\n",
        "\n",
        "    # Check for errors\n",
        "    if response.error.message:\n",
        "        raise Exception(f'Error occurred: {response.error.message}')\n",
        "\n",
        "    # Extract the detected text\n",
        "    extracted_text = response.full_text_annotation.text\n",
        "    return extracted_text.strip()  # Remove leading/trailing whitespace\n",
        "\n",
        "def describe_scene_with_gemini(labels, image_path):\n",
        "    \"\"\"Generate a scene description using Gemini AI or extract text if Paper/Document is detected.\"\"\"\n",
        "    # Prepare the prompt with labels\n",
        "    labels_str = \", \".join(labels)\n",
        "    print(f\"Labels detected in the image: {labels_str}\")\n",
        "\n",
        "    if \"Paper\" in labels_str or \"Document\" in labels_str:\n",
        "        # Extract text from the image\n",
        "        extracted_text = extract_text_from_image(image_path)\n",
        "        description = f\"The scene contains a document. The text extracted from the document is: {extracted_text}\"\n",
        "    else:\n",
        "        prompt = f\"Form a small sentence like 'person XXXX detected' or explain the scene with these words: {labels_str}\"\n",
        "\n",
        "        # Set up the request body for Gemini API\n",
        "        url = f'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_API_KEY}'\n",
        "        data = {\n",
        "            \"contents\": [{\n",
        "                \"parts\": [{\n",
        "                    \"text\": prompt\n",
        "                }]\n",
        "            }]\n",
        "        }\n",
        "\n",
        "        # Send the request to Gemini API\n",
        "        response = requests.post(url, json=data)\n",
        "\n",
        "        # Check if the response is successful\n",
        "        if response.status_code == 200:\n",
        "            response_data = response.json()\n",
        "            description = response_data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "        else:\n",
        "            raise Exception(f\"Error occurred in Gemini API: {response.text}\")\n",
        "\n",
        "    return description\n",
        "\n",
        "def extract_frames_from_video(video_path, frame_interval):\n",
        "    \"\"\"Extract frames from video at a specific interval (in seconds).\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_rate = cap.get(cv2.CAP_PROP_FPS)  # Get the frame rate of the video\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "    success, frame = cap.read()\n",
        "\n",
        "    while success:\n",
        "        # Calculate the frame number to extract based on frame_interval\n",
        "        if frame_count % int(frame_rate * frame_interval) == 0:  # Every `frame_interval` seconds\n",
        "            frame_filename = f'/tmp/frame_{frame_count}.jpg'\n",
        "            cv2.imwrite(frame_filename, frame)  # Save frame as an image\n",
        "            frames.append(frame_filename)\n",
        "\n",
        "        success, frame = cap.read()\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "def main(video_path, frame_interval):\n",
        "    # Extract frames from the video at the specified interval\n",
        "    frames = extract_frames_from_video(video_path, frame_interval)\n",
        "\n",
        "    # Process each frame\n",
        "    for frame_path in frames:\n",
        "        # Get labels from the frame using Google Vision API\n",
        "        labels = get_labels_from_image(frame_path)\n",
        "\n",
        "        # Generate a description using Gemini AI or extract text if document-related labels found\n",
        "        scene_description = describe_scene_with_gemini(labels, frame_path)\n",
        "\n",
        "        # Print the final description\n",
        "        print(\"\\nScene Description:\")\n",
        "        print(scene_description)\n",
        "\n",
        "# Example usage: Change `frame_interval` to any desired interval (in seconds)\n",
        "video_path = \"/content/sample_data/scene_desc3.mp4\"  # Path to the video file\n",
        "frame_interval = 5  # Set this to the desired interval (e.g., 5 seconds)\n",
        "main(video_path, frame_interval)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Wkoo8Sqns1B",
        "outputId": "b9a0ed5e-4a17-4679-ed91-245ce6bea7cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels detected in the image: Plaid\n",
            "\n",
            "Scene Description:\n",
            "Plaid pattern detected.\n",
            "\n",
            "Labels detected in the image: Ceiling, Hall, Chair, Crowd\n",
            "\n",
            "Scene Description:\n",
            "Crowd detected in hall ceiling, chair unoccupied.\n",
            "\n",
            "Labels detected in the image: Chair, Crowd, Convention\n",
            "\n",
            "Scene Description:\n",
            "Crowd detected at convention chair.\n",
            "\n",
            "Labels detected in the image: Photograph, White, Text, Paper, Number, Paper Product, Document, Ink\n",
            "\n",
            "Scene Description:\n",
            "The scene contains a document. The text extracted from the document is: Case Study: Maria's Journey to Global Success\n",
            "Background:\n",
            "Maria, a young professional from Brazil, embarked on a journey to harness the potential of\n",
            "English as a tool for personal and career growth Born and raised in a Portuguese-speaking\n",
            "environment. Maria recognized the need to overcome language barriers to access global\n",
            "opportunities.\n",
            "Challenges:\n",
            "Initially, Maria faced challenges in pursuing advanced education, engaging in international\n",
            "business discussions, and connecting with professionals from diverse backgrounds. The\n",
            "language barrier posed a significant hurdle, limiting her ability to participate fully in the\n",
            "globalized professional landscape\n",
            "English Proficiency as a Catalyst:\n",
            "Recognizing the importance of English proficiency, Maria dedicated herself to learning the\n",
            "language. She enrolled in English language courses, participated in language exchange\n",
            "programs, and immersed herself in English-speaking environments. Over time, Maria's\n",
            "proficiency in English became a catalyst for personal and professional growth.\n",
            "Global Education:\n",
            "Equipped with English proficiency, Maria gained admission to a prestigious international\n",
            "business program. Her ability to understand lectures, collaborate with peers from different\n",
            "countries, and contribute effectively to discussions opened doors to a world-class education\n",
            "that would have been maccessible without a strong command of the English language.\n",
            "Professional Advancement:\n",
            "Upon completing her education, Maria entered the global job market. Her English proficiency\n",
            "became a key asset, allowing her to communicate seamlessly with colleagues, clients, and\n",
            "stakeholders from diverse cultural backgrounds. Maria's ability to articulate ideas, negotiate\n",
            "contracts, and participate in international conferences propelled her career forward, earning her\n",
            "recognition as a global professional.\n",
            "Cultural Integration:\n",
            "English not only served as a tool for professional success but also facilitated cultural\n",
            "integration. Maria engaged with a global network, building relationships with professionals\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "import joblib\n",
        "import torch\n",
        "import numpy as np\n",
        "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
        "from PIL import Image\n",
        "from google.cloud import vision\n",
        "\n",
        "# Set up the Google Cloud Vision API client\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/sample_data/vision-api-450017-cbe451515384.json'\n",
        "\n",
        "client = vision.ImageAnnotatorClient()\n",
        "\n",
        "GEMINI_API_KEY = 'AIzaSyBjuQHU8GYEPm9Fj0Rrna26-E6zdwXAhLg'\n",
        "\n",
        "# Load the trained model for face recognition\n",
        "MODEL_SAVE_PATH = \"/content/sample_data/face_recognition_model.pkl\"\n",
        "model_data = joblib.load(MODEL_SAVE_PATH)\n",
        "classifier = model_data[\"model\"]\n",
        "label_encoder = model_data[\"encoder\"]\n",
        "\n",
        "# Load FaceNet and MTCNN for face recognition\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "facenet_model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "mtcnn = MTCNN(image_size=160, margin=20, keep_all=False)\n",
        "\n",
        "def get_labels_from_image(image_path):\n",
        "    \"\"\"Use Google Vision API to get labels from the image.\"\"\"\n",
        "    with open(image_path, 'rb') as image_file:\n",
        "        content = image_file.read()\n",
        "\n",
        "    # Prepare the image for Vision API\n",
        "    image = vision.Image(content=content)\n",
        "\n",
        "    # Send the request to the API for label detection\n",
        "    response = client.label_detection(image=image)\n",
        "\n",
        "    # Check for errors\n",
        "    if response.error.message:\n",
        "        raise Exception(f'Error occurred: {response.error.message}')\n",
        "\n",
        "    # Extract the labels from the response\n",
        "    labels = [label.description for label in response.label_annotations]\n",
        "\n",
        "    return labels\n",
        "\n",
        "def extract_text_from_image(image_path):\n",
        "    \"\"\"Extract text from an image using Google Vision's document text detection.\"\"\"\n",
        "    with open(image_path, 'rb') as image_file:\n",
        "        content = image_file.read()\n",
        "\n",
        "    # Prepare the image for Vision API\n",
        "    image = vision.Image(content=content)\n",
        "\n",
        "    # Use document text detection for extracting text\n",
        "    response = client.document_text_detection(image=image)\n",
        "\n",
        "    # Check for errors\n",
        "    if response.error.message:\n",
        "        raise Exception(f'Error occurred: {response.error.message}')\n",
        "\n",
        "    # Extract the detected text\n",
        "    extracted_text = response.full_text_annotation.text\n",
        "    return extracted_text.strip()  # Remove leading/trailing whitespace\n",
        "\n",
        "def extract_embeddings(image_path):\n",
        "    \"\"\"Extract embeddings using FaceNet from the image.\"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    face = mtcnn(image)\n",
        "    if face is not None:\n",
        "        face = face.unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            embedding = facenet_model(face)\n",
        "        return embedding.cpu().numpy().flatten()\n",
        "    return None\n",
        "\n",
        "def recognize_face(embedding):\n",
        "    \"\"\"Recognize the face by comparing embeddings with the classifier.\"\"\"\n",
        "    if embedding is not None:\n",
        "        probs = classifier.predict_proba([embedding])[0]  # Get class probabilities\n",
        "        max_prob = max(probs)  # Get the highest probability class\n",
        "\n",
        "        if max_prob < 0.50:  # Set threshold (adjust based on testing)\n",
        "            return \"Unknown person\", max_prob\n",
        "        else:\n",
        "            prediction = np.argmax(probs)  # Get class with highest probability\n",
        "            person_name = label_encoder.inverse_transform([prediction])[0]\n",
        "            return person_name, max_prob\n",
        "    else:\n",
        "        return \"No face detected\", 0\n",
        "\n",
        "def describe_scene_with_gemini(labels, image_path):\n",
        "    \"\"\"Generate a scene description using Gemini AI or extract text if Paper/Document is detected.\"\"\"\n",
        "    labels_str = \", \".join(labels)\n",
        "    print(f\"Labels detected in the image: {labels_str}\")\n",
        "\n",
        "    if \"Paper\" in labels_str or \"Document\" in labels_str:\n",
        "        # Extract text from the image\n",
        "        extracted_text = extract_text_from_image(image_path)\n",
        "        description = f\"The scene contains a document. The text extracted from the document is: {extracted_text}\"\n",
        "    else:\n",
        "        if (labels[-1] in \"srikar charan mohith\"):\n",
        "          prompt = f\"Form a small sentence like '{labels[-1]} detected' or explain the scene with these words: {labels_str}\"\n",
        "        else:\n",
        "          prompt = f\"Form a small sentence like 'person XXXX detected' or explain the scene with these words: {labels_str}\"\n",
        "\n",
        "        # Set up the request body for Gemini API\n",
        "        url = f'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_API_KEY}'\n",
        "        data = {\n",
        "            \"contents\": [{\n",
        "                \"parts\": [{\n",
        "                    \"text\": prompt\n",
        "                }]\n",
        "            }]\n",
        "        }\n",
        "\n",
        "        # Send the request to Gemini API\n",
        "        response = requests.post(url, json=data)\n",
        "\n",
        "        # Check if the response is successful\n",
        "        if response.status_code == 200:\n",
        "            response_data = response.json()\n",
        "            description = response_data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "        else:\n",
        "            raise Exception(f\"Error occurred in Gemini API: {response.text}\")\n",
        "\n",
        "    return description\n",
        "\n",
        "def extract_frames_from_video(video_path, frame_interval):\n",
        "    \"\"\"Extract frames from video at a specific interval (in seconds).\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_rate = cap.get(cv2.CAP_PROP_FPS)  # Get the frame rate of the video\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "    success, frame = cap.read()\n",
        "\n",
        "    while success:\n",
        "        # Calculate the frame number to extract based on frame_interval\n",
        "        if frame_count % int(frame_rate * frame_interval) == 0:  # Every `frame_interval` seconds\n",
        "            frame_filename = f'/tmp/frame_{frame_count}.jpg'\n",
        "            cv2.imwrite(frame_filename, frame)  # Save frame as an image\n",
        "            frames.append(frame_filename)\n",
        "\n",
        "        success, frame = cap.read()\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "def main(video_path, frame_interval):\n",
        "    # Extract frames from the video at the specified interval\n",
        "    frames = extract_frames_from_video(video_path, frame_interval)\n",
        "\n",
        "    # Process each frame\n",
        "    for frame_path in frames:\n",
        "        #print(f\"\\nProcessing frame: {frame_path}\")\n",
        "\n",
        "        # Recognize the face in the frame\n",
        "        embedding = extract_embeddings(frame_path)\n",
        "        person_name, confidence = recognize_face(embedding)\n",
        "        if (confidence<0.50)\n",
        "\n",
        "        # Get labels from the frame using Google Vision API\n",
        "        labels = get_labels_from_image(frame_path)+[person_name]\n",
        "\n",
        "        # Generate a description using Gemini AI or extract text if document-related labels found\n",
        "        scene_description = describe_scene_with_gemini(labels, frame_path)\n",
        "\n",
        "\n",
        "\n",
        "        # Print the final description and face recognition result\n",
        "        print(\"\\nScene Description:\")\n",
        "        print(scene_description)\n",
        "        #print(f\"\\nFace Recognition Result: {person_name} (Confidence: {confidence:.2f})\")\n",
        "\n",
        "# Example usage: Change `frame_interval` to any desired interval (in seconds)\n",
        "video_path = \"/content/sample_data/scene_desc3.mp4\"  # Path to the video file\n",
        "frame_interval = 5  # Set this to the desired interval (e.g., 5 seconds)\n",
        "main(video_path, frame_interval)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDgGAJ2dwiE1",
        "outputId": "156a484a-cce1-42dc-80df-2ce31792d768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels detected in the image: Plaid, srikar\n",
            "\n",
            "Scene Description:\n",
            "Srikar detected the plaid pattern in the shadows.\n",
            "\n",
            "Labels detected in the image: Ceiling, Hall, Chair, Crowd, srikar\n",
            "\n",
            "Scene Description:\n",
            "Srikar detected, amidst the crowd in the hall, from his chair beneath the ceiling.\n",
            "\n",
            "Labels detected in the image: Chair, Crowd, Convention, dheeraj\n",
            "\n",
            "Scene Description:\n",
            "Dheeraj detected in crowded convention chair.\n",
            "\n",
            "Labels detected in the image: Photograph, White, Text, Paper, Number, Paper Product, Document, Ink, unknown\n",
            "\n",
            "Scene Description:\n",
            "The scene contains a document. The text extracted from the document is: Case Study: Maria's Journey to Global Success\n",
            "Background:\n",
            "Maria, a young professional from Brazil, embarked on a journey to harness the potential of\n",
            "English as a tool for personal and career growth Born and raised in a Portuguese-speaking\n",
            "environment. Maria recognized the need to overcome language barriers to access global\n",
            "opportunities.\n",
            "Challenges:\n",
            "Initially, Maria faced challenges in pursuing advanced education, engaging in international\n",
            "business discussions, and connecting with professionals from diverse backgrounds. The\n",
            "language barrier posed a significant hurdle, limiting her ability to participate fully in the\n",
            "globalized professional landscape\n",
            "English Proficiency as a Catalyst:\n",
            "Recognizing the importance of English proficiency, Maria dedicated herself to learning the\n",
            "language. She enrolled in English language courses, participated in language exchange\n",
            "programs, and immersed herself in English-speaking environments. Over time, Maria's\n",
            "proficiency in English became a catalyst for personal and professional growth.\n",
            "Global Education:\n",
            "Equipped with English proficiency, Maria gained admission to a prestigious international\n",
            "business program. Her ability to understand lectures, collaborate with peers from different\n",
            "countries, and contribute effectively to discussions opened doors to a world-class education\n",
            "that would have been maccessible without a strong command of the English language.\n",
            "Professional Advancement:\n",
            "Upon completing her education, Maria entered the global job market. Her English proficiency\n",
            "became a key asset, allowing her to communicate seamlessly with colleagues, clients, and\n",
            "stakeholders from diverse cultural backgrounds. Maria's ability to articulate ideas, negotiate\n",
            "contracts, and participate in international conferences propelled her career forward, earning her\n",
            "recognition as a global professional.\n",
            "Cultural Integration:\n",
            "English not only served as a tool for professional success but also facilitated cultural\n",
            "integration. Maria engaged with a global network, building relationships with professionals\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "from gtts import gTTS\n",
        "import requests\n",
        "import json\n",
        "import joblib\n",
        "import torch\n",
        "import numpy as np\n",
        "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
        "from PIL import Image\n",
        "from google.cloud import vision\n",
        "\n",
        "# Set up the Google Cloud Vision API client\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/sample_data/vision-api-450017-cbe451515384.json'\n",
        "\n",
        "client = vision.ImageAnnotatorClient()\n",
        "\n",
        "GEMINI_API_KEY = 'AIzaSyBjuQHU8GYEPm9Fj0Rrna26-E6zdwXAhLg'\n",
        "\n",
        "# Load the trained model for face recognition\n",
        "MODEL_SAVE_PATH = \"/content/sample_data/face_recognition_model.pkl\"\n",
        "model_data = joblib.load(MODEL_SAVE_PATH)\n",
        "classifier = model_data[\"model\"]\n",
        "label_encoder = model_data[\"encoder\"]\n",
        "\n",
        "# Load FaceNet and MTCNN for face recognition\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "facenet_model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "mtcnn = MTCNN(image_size=160, margin=20, keep_all=False)\n",
        "\n",
        "def get_labels_from_image(image_path):\n",
        "    \"\"\"Use Google Vision API to get labels from the image.\"\"\"\n",
        "    with open(image_path, 'rb') as image_file:\n",
        "        content = image_file.read()\n",
        "\n",
        "    # Prepare the image for Vision API\n",
        "    image = vision.Image(content=content)\n",
        "\n",
        "    # Send the request to the API for label detection\n",
        "    response = client.label_detection(image=image)\n",
        "\n",
        "    # Check for errors\n",
        "    if response.error.message:\n",
        "        raise Exception(f'Error occurred: {response.error.message}')\n",
        "\n",
        "    # Extract the labels from the response\n",
        "    labels = [label.description for label in response.label_annotations]\n",
        "\n",
        "    return labels\n",
        "\n",
        "def extract_text_from_image(image_path):\n",
        "    \"\"\"Extract text from an image using Google Vision's document text detection.\"\"\"\n",
        "    with open(image_path, 'rb') as image_file:\n",
        "        content = image_file.read()\n",
        "\n",
        "    # Prepare the image for Vision API\n",
        "    image = vision.Image(content=content)\n",
        "\n",
        "    # Use document text detection for extracting text\n",
        "    response = client.document_text_detection(image=image)\n",
        "\n",
        "    # Check for errors\n",
        "    if response.error.message:\n",
        "        raise Exception(f'Error occurred: {response.error.message}')\n",
        "\n",
        "    # Extract the detected text\n",
        "    extracted_text = response.full_text_annotation.text\n",
        "    return extracted_text.strip()  # Remove leading/trailing whitespace\n",
        "\n",
        "def extract_embeddings(image_path):\n",
        "    \"\"\"Extract embeddings using FaceNet from the image.\"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    face = mtcnn(image)\n",
        "    if face is not None:\n",
        "        face = face.unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            embedding = facenet_model(face)\n",
        "        return embedding.cpu().numpy().flatten()\n",
        "    return None\n",
        "\n",
        "def recognize_face(embedding):\n",
        "    \"\"\"Recognize the face by comparing embeddings with the classifier.\"\"\"\n",
        "    if embedding is not None:\n",
        "        probs = classifier.predict_proba([embedding])[0]  # Get class probabilities\n",
        "        max_prob = max(probs)  # Get the highest probability class\n",
        "\n",
        "        if max_prob < 0.50:  # Set threshold (adjust based on testing)\n",
        "            return \"Unknown person\", max_prob\n",
        "        else:\n",
        "            prediction = np.argmax(probs)  # Get class with highest probability\n",
        "            person_name = label_encoder.inverse_transform([prediction])[0]\n",
        "            return person_name, max_prob\n",
        "    else:\n",
        "        return \"No face detected\", 0\n",
        "\n",
        "def describe_scene_with_gemini(labels, image_path):\n",
        "    \"\"\"Generate a scene description using Gemini AI or extract text if Paper/Document is detected.\"\"\"\n",
        "    labels_str = \", \".join(labels)\n",
        "    #print(f\"Labels detected in the image: {labels_str}\")\n",
        "\n",
        "    if \"Paper\" in labels_str or \"Document\" in labels_str:\n",
        "        # Extract text from the image\n",
        "        extracted_text = extract_text_from_image(image_path)\n",
        "        description = f\"The scene contains a document. The text extracted from the document is: {extracted_text}\"\n",
        "    else:\n",
        "        if (labels[-1] in \"srikar charan mohith\"):\n",
        "          prompt = f\"Form a small sentence like '{labels[-1]} detected' : {labels_str}\"\n",
        "        else:\n",
        "          prompt = f\"Form a small sentence like 'person doing this work with these words: {labels_str}\"\n",
        "\n",
        "        # Set up the request body for Gemini API\n",
        "        url = f'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_API_KEY}'\n",
        "        data = {\n",
        "            \"contents\": [{\n",
        "                \"parts\": [{\n",
        "                    \"text\": prompt\n",
        "                }]\n",
        "            }]\n",
        "        }\n",
        "\n",
        "        # Send the request to Gemini API\n",
        "        response = requests.post(url, json=data)\n",
        "\n",
        "        # Check if the response is successful\n",
        "        if response.status_code == 200:\n",
        "            response_data = response.json()\n",
        "            description = response_data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "        else:\n",
        "            raise Exception(f\"Error occurred in Gemini API: {response.text}\")\n",
        "\n",
        "    return description\n",
        "\n",
        "def text_to_speech(text, output_filename):\n",
        "    \"\"\"Convert the text to speech and save it as a .wav file.\"\"\"\n",
        "    tts = gTTS(text, lang='en',slow=False)\n",
        "    tts.save(output_filename)\n",
        "    print(f\"Audio file saved as {output_filename}\")\n",
        "\n",
        "def main():\n",
        "    # Extract frames from the video at the specified interval\n",
        "    frames = \"/content/sample_data/image1.jpg\"\n",
        "    embedding = extract_embeddings(frames)\n",
        "    person_name, confidence = recognize_face(embedding)\n",
        "    if (confidence<0.50):\n",
        "      person_name=\"unknown\"\n",
        "\n",
        "    # Get labels from the frame using Google Vision API\n",
        "    labels = get_labels_from_image(frames)+[person_name]\n",
        "\n",
        "    # Generate a description using Gemini AI or extract text if document-related labels found\n",
        "    scene_description = describe_scene_with_gemini(labels, frames)\n",
        "    # Print the final description and face recognition result\n",
        "    text_to_speech(scene_description, \"/content/sample_data/  .wav\")\n",
        "    print(\"\\nScene Description:\")\n",
        "    print(scene_description)\n",
        "    #print(f\"\\nFace Recognition Result: {person_name} (Confidence: {confidence:.2f})\")\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLUN9MhaUutq",
        "outputId": "29d06e99-d9c3-450b-aab5-5c009ed85e92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio file saved as /content/sample_data/  .wav\n",
            "\n",
            "Scene Description:\n",
            "Pedestrians cross the asphalt road's zebra crossing.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "from gtts import gTTS\n",
        "import requests\n",
        "import json\n",
        "import joblib\n",
        "import torch\n",
        "import numpy as np\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
        "from PIL import Image\n",
        "from google.cloud import vision\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "\n",
        "# Set up the Google Cloud Vision API client\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/sample_data/vision-api-450017-cbe451515384.json'\n",
        "\n",
        "client = vision.ImageAnnotatorClient()\n",
        "\n",
        "GEMINI_API_KEY = 'AIzaSyBjuQHU8GYEPm9Fj0Rrna26-E6zdwXAhLg'\n",
        "print(\"executing\")\n",
        "# Load the trained model for face recognition\n",
        "MODEL_SAVE_PATH = \"/content/sample_data/face_recognition_model.pkl\"\n",
        "model_data = joblib.load(MODEL_SAVE_PATH)\n",
        "classifier = model_data[\"model\"]\n",
        "label_encoder = model_data[\"encoder\"]\n",
        "\n",
        "# Load FaceNet and MTCNN for face recognition\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "facenet_model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "mtcnn = MTCNN(image_size=160, margin=20, keep_all=False)\n",
        "\n",
        "# Google Drive API Authentication\n",
        "SERVICE_ACCOUNT_FILE = '/content/sample_data/southern-shade-450017-n1-51653cac7517.json'\n",
        "SCOPES = ['https://www.googleapis.com/auth/drive.file', 'https://www.googleapis.com/auth/drive']\n",
        "FOLDER_ID = '1IqwZ7rO690pPITRP5YTH1QWPh7Ip9VY2'  # Replace with your folder ID\n",
        "\n",
        "def authenticate():\n",
        "    from google.oauth2.service_account import Credentials\n",
        "    credentials = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
        "    return credentials\n",
        "\n",
        "def get_file_from_drive(filename, folder_id):\n",
        "    creds = authenticate()\n",
        "    service = build('drive', 'v3', credentials=creds)\n",
        "\n",
        "    # Search for the file in the specified folder\n",
        "    query = f\"'{folder_id}' in parents and name = '{filename}'\"\n",
        "    results = service.files().list(q=query, fields=\"files(id, name)\").execute()\n",
        "    items = results.get('files', [])\n",
        "\n",
        "    if not items:\n",
        "        print(f\"File {filename} not found in the folder.\")\n",
        "        return None, None  # Return None if file is not found\n",
        "\n",
        "    # Get the file ID\n",
        "    file_id = items[0]['id']\n",
        "\n",
        "    # Download the file to the local system\n",
        "    request = service.files().get_media(fileId=file_id)\n",
        "    file_path = f\"/content/{filename}\"  # Save the file locally in the '/content' folder\n",
        "\n",
        "    # Use MediaIoBaseDownload to download the file\n",
        "    fh = io.FileIO(file_path, 'wb')\n",
        "    downloader = MediaIoBaseDownload(fh, request)\n",
        "    done = False\n",
        "    while not done:\n",
        "        status, done = downloader.next_chunk()\n",
        "        print(f\"Download {filename} - {int(status.progress() * 100)}%.\")\n",
        "\n",
        "    print(f\"Downloaded {filename} to {file_path}\")\n",
        "    return file_path, file_id\n",
        "\n",
        "def upload_file_to_drive(file_path, mime_type='audio/mp3'):\n",
        "    creds = authenticate()\n",
        "    service = build('drive', 'v3', credentials=creds)\n",
        "    file_metadata = {\n",
        "        'name': os.path.basename(file_path),\n",
        "        'parents': [FOLDER_ID]\n",
        "    }\n",
        "    media = MediaFileUpload(file_path, mimetype=mime_type)\n",
        "    file = service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
        "    print(f\"File uploaded successfully with ID: {file.get('id')}\")\n",
        "    return file.get('id')\n",
        "\n",
        "def rename_file_in_drive(file_id, new_name):\n",
        "    creds = authenticate()\n",
        "    service = build('drive', 'v3', credentials=creds)\n",
        "    file_metadata = {'name': new_name}\n",
        "    service.files().update(fileId=file_id, body=file_metadata).execute()\n",
        "    print(f\"File renamed successfully to: {new_name}\")\n",
        "\n",
        "def delete_file_from_drive(file_name):\n",
        "    creds = authenticate()\n",
        "    service = build('drive', 'v3', credentials=creds)\n",
        "    query = f\"'{FOLDER_ID}' in parents and name = '{file_name}'\"\n",
        "    results = service.files().list(q=query, fields=\"files(id, name)\").execute()\n",
        "    items = results.get('files', [])\n",
        "    if items:\n",
        "        for item in items:\n",
        "            file_id = item['id']\n",
        "            service.files().delete(fileId=file_id).execute()\n",
        "            print(f\"Deleted file: {file_name}\")\n",
        "\n",
        "def get_labels_from_image(image_path):\n",
        "    \"\"\"Use Google Vision API to get labels from the image.\"\"\"\n",
        "    with open(image_path, 'rb') as image_file:\n",
        "        content = image_file.read()\n",
        "    image = vision.Image(content=content)\n",
        "    response = client.label_detection(image=image)\n",
        "    if response.error.message:\n",
        "        raise Exception(f'Error occurred: {response.error.message}')\n",
        "    labels = [label.description for label in response.label_annotations]\n",
        "    return labels\n",
        "\n",
        "def extract_text_from_image(image_path):\n",
        "    \"\"\"Extract text from an image using Google Vision's document text detection.\"\"\"\n",
        "    with open(image_path, 'rb') as image_file:\n",
        "        content = image_file.read()\n",
        "    image = vision.Image(content=content)\n",
        "    response = client.document_text_detection(image=image)\n",
        "    if response.error.message:\n",
        "        raise Exception(f'Error occurred: {response.error.message}')\n",
        "    extracted_text = response.full_text_annotation.text\n",
        "    return extracted_text.strip()\n",
        "\n",
        "def extract_embeddings(image_path):\n",
        "    \"\"\"Extract embeddings using FaceNet from the image.\"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    face = mtcnn(image)\n",
        "    if face is not None:\n",
        "        face = face.unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            embedding = facenet_model(face)\n",
        "        return embedding.cpu().numpy().flatten()\n",
        "    return None\n",
        "\n",
        "def recognize_face(embedding):\n",
        "    \"\"\"Recognize the face by comparing embeddings with the classifier.\"\"\"\n",
        "    if embedding is not None:\n",
        "        probs = classifier.predict_proba([embedding])[0]  # Get class probabilities\n",
        "        max_prob = max(probs)  # Get the highest probability class\n",
        "        if max_prob < 0.50:  # Set threshold (adjust based on testing)\n",
        "            return \"Unknown person\", max_prob\n",
        "        else:\n",
        "            prediction = np.argmax(probs)  # Get class with highest probability\n",
        "            person_name = label_encoder.inverse_transform([prediction])[0]\n",
        "            return person_name, max_prob\n",
        "    else:\n",
        "        return \"No face detected\", 0\n",
        "\n",
        "def describe_scene_with_gemini(labels, image_path):\n",
        "    \"\"\"Generate a scene description using Gemini AI or extract text if Paper/Document is detected.\"\"\"\n",
        "    labels_str = \", \".join(labels)\n",
        "    if \"Paper\" in labels_str or \"Document\" in labels_str:\n",
        "        extracted_text = extract_text_from_image(image_path)\n",
        "        description = f\"The scene contains a document. The text extracted from the document is: {extracted_text}\"\n",
        "    else:\n",
        "        prompt = f\"Form a small descriptive text which can be read in around 22 seconds in simple english that gives out text like 'person doing this work with these words: {labels_str}'\"\n",
        "        url = f'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_API_KEY}'\n",
        "        data = {\n",
        "            \"contents\": [{\n",
        "                \"parts\": [{\n",
        "                    \"text\": prompt\n",
        "                }]\n",
        "            }]\n",
        "        }\n",
        "        response = requests.post(url, json=data)\n",
        "        if response.status_code == 200:\n",
        "            response_data = response.json()\n",
        "            description = response_data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "        else:\n",
        "            raise Exception(f\"Error occurred in Gemini API: {response.text}\")\n",
        "    return description\n",
        "\n",
        "def text_to_speech(text, output_filename):\n",
        "    \"\"\"Convert the text to speech and save it as a .wav file.\"\"\"\n",
        "    tts = gTTS(text, lang='en', slow=False)\n",
        "    tts.save(output_filename)\n",
        "    print(f\"Audio file saved as {output_filename}\")\n",
        "\n",
        "\n",
        "# Download the image file from Google Drive\n",
        "image_file_path, image_file_id = get_file_from_drive('image.jpg',FOLDER_ID)  # Make sure the image name is correct\n",
        "print(image_file_path)\n",
        "if image_file_path:\n",
        "  # Extract embeddings from the image\n",
        "  embedding = extract_embeddings(image_file_path)\n",
        "  person_name, confidence = recognize_face(embedding)\n",
        "  if confidence < 0.50:\n",
        "      person_name = \"unknown\"\n",
        "\n",
        "  # Get labels from the image using Google Vision API\n",
        "  labels = get_labels_from_image(image_file_path) + [person_name]\n",
        "  print(labels)\n",
        "  # Generate a description using Gemini AI or extract text if document-related labels found\n",
        "  scene_description = describe_scene_with_gemini(labels, image_file_path)\n",
        "  print(\"\\nScene Description:\")\n",
        "  print(scene_description)\n",
        "\n",
        "  # Delete the existing .wav file from Google Drive\n",
        "  delete_file_from_drive('scene_description.mp3')\n",
        "\n",
        "  # Generate and save the new .wav file\n",
        "  wav_file_path = '/content/sample_data/scene_description.mp3'\n",
        "  text_to_speech(scene_description, wav_file_path)\n",
        "  # Upload the newly generated .wav file to Google Drive\n",
        "  upload_file_to_drive(wav_file_path, mime_type='audio/mp3')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOVOrKNz5Nbt",
        "outputId": "3ee2e56a-35be-4cdd-a6a5-675de38c3696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "executing\n",
            "Download image.jpg - 100%.\n",
            "Downloaded image.jpg to /content/image.jpg\n",
            "/content/image.jpg\n",
            "['Furniture', 'Hall', 'Ceiling', 'Chair', 'Auditorium', 'Convention', 'Community centre', 'mohith']\n",
            "\n",
            "Scene Description:\n",
            "Mohith worked on the hall's ceiling, installing new chairs for the upcoming convention.  The community center's auditorium needed furniture upgrades, and Mohith was the one to do it.\n",
            "\n",
            "Deleted file: scene_description.mp3\n",
            "Deleted file: scene_description.mp3\n",
            "Deleted file: scene_description.mp3\n",
            "Deleted file: scene_description.mp3\n",
            "Audio file saved as /content/sample_data/scene_description.mp3\n",
            "File uploaded successfully with ID: 1BONarBIicXQPdNLr7410v0-yfCiHBmeP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_14SXy0JYfHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ilioi_s6by5-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}